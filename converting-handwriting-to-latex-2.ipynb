{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63599,"databundleVersionId":6947127,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom PIL import Image\nimport torch\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset\nimport re\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3fdaf3b9-8f1d-4129-ad29-91e6c1c8fc90","_cell_guid":"f7a7c439-47d4-4b95-8ad2-59a2ad17d411","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:37.915802Z","iopub.execute_input":"2025-04-09T04:57:37.916155Z","iopub.status.idle":"2025-04-09T04:57:37.920948Z","shell.execute_reply.started":"2025-04-09T04:57:37.916079Z","shell.execute_reply":"2025-04-09T04:57:37.919875Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_code = pd.read_csv(\"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\")\ntrain_code.head()","metadata":{"_uuid":"dc123c28-e0a1-4505-8342-4e5f64c3a7d5","_cell_guid":"150645e9-d6cf-4a20-8e98-56f26a7b0310","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:42.163587Z","iopub.execute_input":"2025-04-09T04:57:42.163965Z","iopub.status.idle":"2025-04-09T04:57:42.392505Z","shell.execute_reply.started":"2025-04-09T04:57:42.163931Z","shell.execute_reply":"2025-04-09T04:57:42.391545Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_code.iloc[0, 1]","metadata":{"_uuid":"8533f045-1d58-43fd-9cea-8903e3c7a7ab","_cell_guid":"5edbcf63-3c0c-4717-901d-0165c4be4316","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:43.585961Z","iopub.execute_input":"2025-04-09T04:57:43.586336Z","iopub.status.idle":"2025-04-09T04:57:43.592204Z","shell.execute_reply.started":"2025-04-09T04:57:43.586305Z","shell.execute_reply":"2025-04-09T04:57:43.591421Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_code.formula.values","metadata":{"_uuid":"a52ab979-bc80-4cc4-acbd-2aaa978e3a59","_cell_guid":"2ae2a7e8-4120-4b7c-af60-86f4488ee8c5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:44.730660Z","iopub.execute_input":"2025-04-09T04:57:44.730956Z","iopub.status.idle":"2025-04-09T04:57:44.736254Z","shell.execute_reply.started":"2025-04-09T04:57:44.730933Z","shell.execute_reply":"2025-04-09T04:57:44.735413Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"3e6dc54e-df05-4174-87b5-c909cd2a30d4","_cell_guid":"e606a9ae-1c91-4f4f-91e1-054de817cbbf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LatexTokenizer:\n    def __init__(self, latex_strings_corpus, max_seq_len=100):\n        self.max_seq_len = max_seq_len\n        self.vocab = self.build_vocab(latex_strings_corpus)\n        self.token_to_idx = self.vocab\n        self.idx_to_token = self.create_inverse_vocab(self.vocab)\n\n    def tokenize(self, latex_string):\n        token_pattern = r\"\\\\[a-zA-Z]+|[{}]|[0-9]+|[^\\s]\"\n        tokens = re.findall(token_pattern, latex_string)\n        return tokens\n\n    def create_inverse_vocab(self, vocab):\n        inv_dict = {}\n        for key, value in vocab.items():\n            inv_dict[value] = key\n        return inv_dict\n    \n    def build_vocab(self, latex_strings):\n        all_tokens = []\n        for latex_string in latex_strings:\n            tokens = self.tokenize(latex_string)\n            all_tokens.extend(tokens)\n            \n        all_tokens.extend([\"<|SOS|>\", \"<|EOS|>\", \"<|PAD|>\"])\n        all_tokens_set = set(all_tokens)\n        vocab = {}\n        for idx, item in enumerate(all_tokens_set):\n            vocab[item] = idx\n\n        return vocab\n\n    def get_vocab_size(self):\n        return len(self.vocab)\n\n    def encode(self, latex_string):\n        tokens = self.tokenize(latex_string)\n        encoded_tokens = [self.token_to_idx[\"<|SOS|>\"]] + [self.token_to_idx.get(token, self.token_to_idx[\"<|PAD|>\"]) for token in tokens] + [self.token_to_idx[\"<|EOS|>\"]]\n\n        if len(encoded_tokens) < self.max_seq_len:\n            encoded_tokens += [self.token_to_idx[\"<|PAD|>\"]] * (self.max_seq_len - len(encoded_tokens))\n        else:\n            encoded_tokens = encoded_tokens[:self.max_seq_len]\n            \n        return encoded_tokens\n\n    def decode(self, encoded_sequence):\n        tokens = [self.idx_to_token[idx] for idx in encoded_sequence if idx != self.token_to_idx[\"<|PAD|>\"]]\n\n        if \"<|SOS|>\" in tokens: tokens.remove(\"<|SOS|>\")\n        if \"<|EOS|>\" in tokens: tokens.remove(\"<|EOS|>\")\n        sequence = \" \".join(tokens)\n        sequence = re.sub(r'\\s+([,.?/!;:\"()_\\']|--)', r'\\1', sequence)\n        return sequence","metadata":{"_uuid":"042e6010-c2de-49bc-82b0-ca1e803fea42","_cell_guid":"a56ded78-edc4-40bd-85e3-3895ace6d18b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:46.916698Z","iopub.execute_input":"2025-04-09T04:57:46.917025Z","iopub.status.idle":"2025-04-09T04:57:46.926494Z","shell.execute_reply.started":"2025-04-09T04:57:46.916996Z","shell.execute_reply":"2025-04-09T04:57:46.925414Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample dataset\nlatex_strings = [\"\\\\frac{a}{b} + c\", \"\\\\sqrt{x^2 + y^2}\"]\n\n# Initialize tokenizer\ntokenizer = LatexTokenizer(latex_strings, max_seq_len=30)\n\n# Build vocabulary from dataset\n# tokenizer.build_vocab(latex_strings)\nprint(\"Vocabulary:\", tokenizer.token_to_idx)\n\n# Encode a single LaTeX string\nencoded_sequence = tokenizer.encode(\"\\\\frac{a}{b} + c\")\nprint(\"Encoded sequence:\", encoded_sequence)\n\n# Decode the sequence back into a string\ndecoded_string = tokenizer.decode(encoded_sequence)\nprint(\"Decoded string:\", decoded_string)","metadata":{"_uuid":"b4f1fb0c-21d2-4270-9567-cb253127c854","_cell_guid":"8d5c829d-a418-4ccf-895b-73bb59f855b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:49.047611Z","iopub.execute_input":"2025-04-09T04:57:49.047965Z","iopub.status.idle":"2025-04-09T04:57:49.054794Z","shell.execute_reply.started":"2025-04-09T04:57:49.047936Z","shell.execute_reply":"2025-04-09T04:57:49.053899Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EquationsDataset(Dataset):\n    def __init__(self, csv_file, image_folder, transform=None, max_seq_len=100):\n        self.data = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.transform = transform\n        self.max_seq_len = max_seq_len\n        self.latex_tokenizer = LatexTokenizer(latex_strings_corpus=self.data.formula.values, max_seq_len=self.max_seq_len)\n        self.vocab_size = self.latex_tokenizer.get_vocab_size()\n        self.vocab = self.latex_tokenizer.vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_name = self.data.iloc[idx, 0]\n        formula = self.data.iloc[idx, 1]\n\n        image_path = os.path.join(self.image_folder, image_name)\n        image = Image.open(image_path).convert(\"L\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        encoded_formula = self.latex_tokenizer.encode(formula)\n\n        return image, torch.tensor(encoded_formula)","metadata":{"_uuid":"8ca180dc-c938-4465-a531-082d718fff03","_cell_guid":"c6559307-b989-412a-8d72-7ee5063f2cae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:49.447706Z","iopub.execute_input":"2025-04-09T04:57:49.448021Z","iopub.status.idle":"2025-04-09T04:57:49.454681Z","shell.execute_reply.started":"2025-04-09T04:57:49.447996Z","shell.execute_reply":"2025-04-09T04:57:49.453683Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\nimage_folder = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\ndataset = EquationsDataset(csv_file=csv_file, image_folder=image_folder,\n                              transform=image_transform, max_seq_len=60)\nimage_tensor, label_tensor = dataset[0]\nprint(\"Image shape:\", image_tensor.shape)       # Example: [3, 224, 224]\nprint(\"Label tensor:\", label_tensor.shape)","metadata":{"_uuid":"bfd716b0-c0e6-4a9c-bfd0-f9e66d4e6cc2","_cell_guid":"08990761-958f-4b33-8c00-4d3855c1ba8d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:52.054636Z","iopub.execute_input":"2025-04-09T04:57:52.054978Z","iopub.status.idle":"2025-04-09T04:57:53.823056Z","shell.execute_reply.started":"2025-04-09T04:57:52.054947Z","shell.execute_reply":"2025-04-09T04:57:53.822076Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\n# Assuming `dataset` is your custom dataset instance\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# Get a single batch of data\nimages, labels = next(iter(dataloader))\n\n# Plot the images and labels\nfig, axes = plt.subplots(1, 4, figsize=(35, 5))  # 1 row, 4 columns\nfor i in range(4):\n    ax = axes[i]\n    image = images[i].permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n    label = labels[i]\n\n    ax.imshow(image)\n    ax.set_title(f\"Label: {label}\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"b330582d-0082-473a-af31-d42d786d0cc5","_cell_guid":"e406215b-9dd7-4f85-8fbe-e50b0eb31246","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:57:54.178868Z","iopub.execute_input":"2025-04-09T04:57:54.179245Z","iopub.status.idle":"2025-04-09T04:57:55.029877Z","shell.execute_reply.started":"2025-04-09T04:57:54.179213Z","shell.execute_reply":"2025-04-09T04:57:55.028897Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.cnn = models.resnet50(pretrained=True)\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove FC layers\n\n    def forward(self, x):\n        return self.cnn(x)  # Output: feature map\n\n# class Decoder(nn.Module):\n#     def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n#         super(Decoder, self).__init__()\n#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n#         self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim)\n#         self.fc = nn.Linear(hidden_dim, vocab_size)\n#         self.attention_layer = nn.Linear(hidden_dim + 2048, hidden_dim)  # Combine feature map and decoder state\n#         self.attention_softmax = nn.Softmax(dim=1)\n\n#     def forward(self, prev_token_idx, encoder_features, prev_hidden_state):\n#         embedded_token = self.embedding(prev_token_idx)\n#         combined_features = torch.cat([encoder_features.flatten(1), prev_hidden_state], dim =-1)\n#         attention_weights = self.attention_softmax(self.attention_layer(combined_features))\n#         context_vector = torch.sum(attention_weights.unsqueeze(-1) * encoder_features.flatten(2), dim=1)\n\n#         lstm_input = torch.cat([embedded_token.unsqueeze(1), context_vector.unsqueeze(1)], dim=-1)\n#         lstm_output, new_hidden_state = self.lstm(lstm_input)\n\n#         token_logits = self.fc(lstm_output.squeeze(1))\n\n#         return token_logits, new_hidden_state\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim + 2048, hidden_dim)  # Input is embedding + context vector\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.attention = nn.Linear(2048 + hidden_dim, 1)  # Attention scoring function\n\n    def forward(self, prev_token_idx, encoder_features, hidden_state, cell_state):\n        # Get token embeddings\n        # Shape: [batch_size, embed_dim]\n        embedded = self.embedding(prev_token_idx)\n        batch_size = prev_token_idx.size(0)\n        \n        # Reshape encoder features\n        # Shape: [batch_size, num_pixels, encoder_dim]\n        encoder_dim = encoder_features.size(1)\n        num_pixels = encoder_features.size(2) * encoder_features.size(3)\n        encoder_features = encoder_features.permute(0, 2, 3, 1).view(batch_size, num_pixels, encoder_dim)\n        \n        # Expand hidden state for attention calculation\n        # Shape: [batch_size, num_pixels, hidden_dim]\n        hidden_expanded = hidden_state.unsqueeze(1).repeat(1, num_pixels, 1)\n        \n        # Calculate attention scores\n        # Shape: [batch_size, num_pixels, 1]\n        attn_inputs = torch.cat([hidden_expanded, encoder_features], dim=2)\n        attn_scores = self.attention(attn_inputs)\n        attn_weights = torch.softmax(attn_scores, dim=1)\n        \n        # Calculate context vector using attention weights\n        # Shape: [batch_size, encoder_dim]\n        context_vector = (encoder_features * attn_weights).sum(dim=1)\n        \n        # Combine embedding and context for LSTM input\n        # Shape: [1, batch_size, embed_dim + encoder_dim]\n        lstm_input = torch.cat([embedded, context_vector], dim=1).unsqueeze(0)\n        \n        # LSTM forward pass\n        # hidden_state shape: [1, batch_size, hidden_dim]\n        # cell_state shape: [1, batch_size, hidden_dim]\n        lstm_output, (new_hidden_state, new_cell_state) = self.lstm(\n            lstm_input, (hidden_state.unsqueeze(0), cell_state.unsqueeze(0))\n        )\n        \n        # Predict next token\n        # Shape: [batch_size, vocab_size]\n        output = self.fc(lstm_output.squeeze(0))\n        \n        return output, new_hidden_state.squeeze(0), new_cell_state.squeeze(0)","metadata":{"_uuid":"8e6b97df-1c7d-4146-9872-1a6dfbfcff7a","_cell_guid":"e7e2a908-ca15-4401-af52-e25b4929c296","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:58:00.801425Z","iopub.execute_input":"2025-04-09T04:58:00.801725Z","iopub.status.idle":"2025-04-09T04:58:00.811410Z","shell.execute_reply.started":"2025-04-09T04:58:00.801702Z","shell.execute_reply":"2025-04-09T04:58:00.810451Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class ImageToLatexModel(nn.Module):\n#     def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n#         super(ImageToLatexModel, self).__init__()\n#         self.encoder = Encoder()  # Use the Encoder class\n#         self.decoder = Decoder(vocab_size, embed_dim, hidden_dim)  # Use the Decoder class\n\n#     def forward(self, image, target_sequence):\n#         \"\"\"\n#         Args:\n#             image: Input image tensor (shape: [batch_size, channels, height, width]).\n#             target_sequence: Ground truth token indices (shape: [batch_size, seq_len]).\n\n#         Returns:\n#             logits: Predicted logits for each token in the sequence.\n#         \"\"\"\n#         batch_size = image.size(0)\n#         seq_len = target_sequence.size(1)\n\n#         # Encode the image\n#         encoder_features = self.encoder(image)  # Shape: [batch_size, channels=2048, height=7, width=7]\n\n#         # Initialize decoder hidden state\n#         hidden_state = torch.zeros(1, batch_size, 512).to(image.device)  # LSTM hidden state\n#         cell_state = torch.zeros(1, batch_size, 512).to(image.device)    # LSTM cell state\n\n#         # Initialize token predictions\n#         logits = []\n\n#         # Iterate through each timestep in the sequence\n#         for t in range(seq_len):\n#             prev_token_idx = target_sequence[:, t]  # Get token index at timestep t\n\n#             # Decode one step\n#             token_logits, (hidden_state, cell_state) = self.decoder(\n#                 prev_token_idx,\n#                 encoder_features,\n#                 hidden_state.squeeze(0)  # Pass hidden state from previous timestep\n#             )\n\n#             logits.append(token_logits)\n\n#         # Stack logits across timesteps\n#         logits = torch.stack(logits, dim=1)  # Shape: [batch_size, seq_len, vocab_size]\n#         return logits\n\n\nclass ImageToLatexModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n        super(ImageToLatexModel, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim)\n        self.vocab_size = vocab_size\n\n    def forward(self, images, target_sequences=None, max_len=100, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Args:\n            images: Input images (shape: [batch_size, channels, height, width])\n            target_sequences: Ground truth sequences (shape: [batch_size, seq_len])\n                             None during inference\n            max_len: Maximum sequence length for inference\n            teacher_forcing_ratio: Probability of using teacher forcing\n        \n        Returns:\n            outputs: Predicted logits for each token\n        \"\"\"\n        batch_size = images.size(0)\n        \n        # Encode images\n        encoder_features = self.encoder(images)\n        \n        # Initialize decoder states\n        hidden_state = torch.zeros(batch_size, 512).to(images.device)\n        cell_state = torch.zeros(batch_size, 512).to(images.device)\n        \n        # Determine sequence length\n        if target_sequences is not None:\n            max_len = target_sequences.size(1) - 1  # Exclude <EOS>\n            \n        # Initialize first input token as <SOS>\n        current_token_idx = torch.ones(batch_size, dtype=torch.long).to(images.device) * dataset.vocab[\"<|SOS|>\"]\n        \n        # Placeholder for outputs\n        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(images.device)\n        \n        # Generate sequence\n        for t in range(max_len):\n            # Decode one step\n            output, hidden_state, cell_state = self.decoder(\n                current_token_idx, encoder_features, hidden_state, cell_state\n            )\n            \n            # Store output\n            outputs[:, t, :] = output\n            \n            # Determine next input token (teacher forcing or predicted)\n            if target_sequences is not None and t < max_len - 1:\n                # During training\n                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n                if teacher_force:\n                    current_token_idx = target_sequences[:, t+1]  # Next token from target\n                else:\n                    current_token_idx = output.argmax(dim=1)  # Predicted token\n            else:\n                # During inference\n                current_token_idx = output.argmax(dim=1)  # Predicted token\n        \n        return outputs","metadata":{"_uuid":"207b7954-155f-429d-abc4-d19f44327ae8","_cell_guid":"b24e9c8e-832f-4016-a711-153e5da8247e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:58:03.573878Z","iopub.execute_input":"2025-04-09T04:58:03.574274Z","iopub.status.idle":"2025-04-09T04:58:03.582831Z","shell.execute_reply.started":"2025-04-09T04:58:03.574244Z","shell.execute_reply":"2025-04-09T04:58:03.581963Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nvocab_size = dataset.vocab_size\nvocab = dataset.vocab\nmodel = ImageToLatexModel(vocab_size=vocab_size)\nprint(model)\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<|PAD|>\"])  # Ignore padding tokens in loss calculation\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"_uuid":"f0c92257-f733-4b1e-9af9-854f237f1800","_cell_guid":"e5e6f3cd-3bde-40d0-bdc5-0700a8aef253","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:58:06.474832Z","iopub.execute_input":"2025-04-09T04:58:06.475174Z","iopub.status.idle":"2025-04-09T04:58:07.782919Z","shell.execute_reply.started":"2025-04-09T04:58:06.475148Z","shell.execute_reply":"2025-04-09T04:58:07.781997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n\n# # Define hyperparameters\n# num_epochs = 10\n# patience = 3  # Number of epochs to wait before early stopping\n# delta = 0.001  # Minimum change to qualify as improvement\n\n# # Move model to device (GPU/CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# # Initialize tracking variables for early stopping\n# best_loss = float('inf')\n# early_stopping_counter = 0\n# best_epoch = 0\n\n# # Training loop\n# for epoch in range(num_epochs):\n#     model.train()\n#     epoch_loss = 0.0\n    \n#     for batch_idx, (images, target_sequences) in enumerate(dataloader):\n#         images = images.to(device)\n#         target_sequences = target_sequences.to(device)\n        \n#         # Forward pass\n#         optimizer.zero_grad()\n#         outputs = model(images, target_sequences)\n        \n#         # Calculate loss (reshape to [batch_size*seq_len, vocab_size])\n#         loss = criterion(\n#             outputs.reshape(-1, vocab_size),\n#             target_sequences[:, 1:].reshape(-1)  # Exclude < SOS > from targets\n#         )\n        \n#         # Backward pass and optimize\n#         loss.backward()\n#         optimizer.step()\n        \n#         epoch_loss += loss.item()\n#         if (batch_idx + 1) % 10 == 0:\n#             print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n    \n#     # Calculate average loss for the epoch\n#     avg_epoch_loss = epoch_loss / len(dataloader)\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_epoch_loss:.4f}\")\n    \n#     # Save checkpoint after each epoch\n#     checkpoint = {\n#         'epoch': epoch,\n#         'model_state_dict': model.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n#         'loss': avg_epoch_loss,\n#         'vocab': dataset.vocab\n#     }\n#     torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n    \n#     # Check if this is the best model so far\n#     if avg_epoch_loss < best_loss - delta:\n#         print(f\"Loss improved from {best_loss:.4f} to {avg_epoch_loss:.4f}. Saving best model...\")\n#         best_loss = avg_epoch_loss\n#         best_epoch = epoch + 1\n#         torch.save(checkpoint, 'best_model.pth')\n#         early_stopping_counter = 0  # Reset counter\n#     else:\n#         early_stopping_counter += 1\n#         print(f\"Loss did not improve. Early stopping counter: {early_stopping_counter}/{patience}\")\n        \n#     # Check if early stopping criteria is met\n#     if early_stopping_counter >= patience:\n#         print(f\"Early stopping triggered after epoch {epoch+1}. Best loss: {best_loss:.4f} at epoch {best_epoch}\")\n#         break\n\n# print(f\"Training complete. Best loss: {best_loss:.4f} at epoch {best_epoch}\")","metadata":{"_uuid":"28b6b3a9-7a3a-4430-929f-c1ce9b6fdee6","_cell_guid":"4a85adcc-7cbe-45ce-80ca-ae80af59bd8e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-08T16:58:28.808462Z","iopub.execute_input":"2025-04-08T16:58:28.808792Z","iopub.status.idle":"2025-04-08T16:58:28.813088Z","shell.execute_reply.started":"2025-04-08T16:58:28.808765Z","shell.execute_reply":"2025-04-08T16:58:28.812075Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# from torch.utils.data import DataLoader, random_split\n\n# # Create train/validation split\n# train_size = int(0.8 * len(dataset))  # 80% for training\n# val_size = len(dataset) - train_size  # 20% for validation\n\n# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# # Create data loaders\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# # Define hyperparameters\n# num_epochs = 10\n# patience = 3  # Number of epochs to wait before early stopping\n# delta = 0.001  # Minimum change to qualify as improvement\n\n# # Move model to device (GPU/CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# # Initialize tracking variables for early stopping\n# best_val_loss = float('inf')\n# early_stopping_counter = 0\n# best_epoch = 0\n\n# # Training loop\n# for epoch in range(num_epochs):\n#     # Training phase\n#     model.train()\n#     train_loss = 0.0\n    \n#     for batch_idx, (images, target_sequences) in enumerate(train_loader):\n#         images = images.to(device)\n#         target_sequences = target_sequences.to(device)\n        \n#         # Forward pass\n#         optimizer.zero_grad()\n#         outputs = model(images, target_sequences)\n        \n#         # Calculate loss\n#         loss = criterion(\n#             outputs.reshape(-1, vocab_size),\n#             target_sequences[:, 1:].reshape(-1)  # Exclude <SOS> from targets\n#         )\n        \n#         # Backward pass and optimize\n#         loss.backward()\n#         optimizer.step()\n        \n#         train_loss += loss.item()\n#         # if (batch_idx + 1) % 10 == 0:\n#         #     print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n    \n#     avg_train_loss = train_loss / len(train_loader)\n    \n#     # Validation phase\n#     model.eval()\n#     val_loss = 0.0\n    \n#     with torch.no_grad():  # No gradients needed for validation\n#         for images, target_sequences in val_loader:\n#             images = images.to(device)\n#             target_sequences = target_sequences.to(device)\n            \n#             # Forward pass\n#             outputs = model(images, target_sequences)\n            \n#             # Calculate loss\n#             loss = criterion(\n#                 outputs.reshape(-1, vocab_size),\n#                 target_sequences[:, 1:].reshape(-1)\n#             )\n            \n#             val_loss += loss.item()\n    \n#     avg_val_loss = val_loss / len(val_loader)\n    \n#     # Print epoch statistics\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n    \n#     # Save checkpoint after each epoch\n#     checkpoint = {\n#         'epoch': epoch,\n#         'model_state_dict': model.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n#         'train_loss': avg_train_loss,\n#         'val_loss': avg_val_loss,\n#         'vocab': dataset.vocab\n#     }\n#     torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n    \n#     # Check if validation loss improved\n#     if avg_val_loss < best_val_loss - delta:\n#         print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving best model...\")\n#         best_val_loss = avg_val_loss\n#         best_epoch = epoch + 1\n#         torch.save(checkpoint, 'best_model.pth')\n#         early_stopping_counter = 0  # Reset counter\n#     else:\n#         early_stopping_counter += 1\n#         print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{patience}\")\n        \n#     # Check if early stopping criteria is met\n#     if early_stopping_counter >= patience:\n#         print(f\"Early stopping triggered after epoch {epoch+1}. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n#         break\n\n# print(f\"Training complete. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n\n\nimport torch\nimport numpy as np\nimport os\nfrom torch.utils.data import DataLoader, random_split\n\n# Create train/validation split\ntrain_size = int(0.8 * len(dataset))  # 80% for training\nval_size = len(dataset) - train_size  # 20% for validation\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Define hyperparameters\nnum_epochs = 10\npatience = 3  # Number of epochs to wait before early stopping\ndelta = 0.001  # Minimum change to qualify as improvement\n\n# Move model to device (GPU/CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Initialize tracking variables for early stopping\nbest_val_loss = float('inf')\nearly_stopping_counter = 0\nbest_epoch = 0\nstart_epoch = 0\n\n# Check if best model checkpoint exists and load it\nbest_model_path = 'best_model.pth'\nif os.path.exists(best_model_path):\n    print(f\"Found existing checkpoint at {best_model_path}. Loading...\")\n    checkpoint = torch.load(best_model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n    best_val_loss = checkpoint['val_loss']\n    best_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming training from epoch {start_epoch}, best validation loss: {best_val_loss:.4f}\")\n\n# Training loop\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    \n    for batch_idx, (images, target_sequences) in enumerate(train_loader):\n        images = images.to(device)\n        target_sequences = target_sequences.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(images, target_sequences)\n        \n        # Calculate loss\n        loss = criterion(\n            outputs.reshape(-1, vocab_size),\n            target_sequences[:, 1:].reshape(-1)  # Exclude < SOS > from targets\n        )\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():  # No gradients needed for validation\n        for images, target_sequences in val_loader:\n            images = images.to(device)\n            target_sequences = target_sequences.to(device)\n            \n            # Forward pass\n            outputs = model(images, target_sequences)\n            \n            # Calculate loss\n            loss = criterion(\n                outputs.reshape(-1, vocab_size),\n                target_sequences[:, 1:].reshape(-1)\n            )\n            \n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    \n    # Print epoch statistics\n    print(f\"Epoch [{epoch+1}/{start_epoch + num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n    \n    # Save checkpoint after each epoch\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': avg_train_loss,\n        'val_loss': avg_val_loss,\n        'vocab': dataset.vocab\n    }\n    torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n    \n    # Check if validation loss improved\n    if avg_val_loss < best_val_loss - delta:\n        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving best model...\")\n        best_val_loss = avg_val_loss\n        best_epoch = epoch + 1\n        torch.save(checkpoint, 'best_model.pth')\n        early_stopping_counter = 0  # Reset counter\n    else:\n        early_stopping_counter += 1\n        print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{patience}\")\n        \n    # Check if early stopping criteria is met\n    if early_stopping_counter >= patience:\n        print(f\"Early stopping triggered after epoch {epoch+1}. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n        break\n\nprint(f\"Training complete. Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")","metadata":{"_uuid":"1036f0c6-d2c7-4f71-aac3-12b77a7cacc2","_cell_guid":"d5471284-82d9-4569-91f7-51da0a777b7b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-08T16:59:55.323997Z","iopub.execute_input":"2025-04-08T16:59:55.324293Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the entire model (architecture + parameters)\n# torch.save(model, 'image_to_latex_full_model.pth')\n\n# # Or, save just the model parameters (recommended approach)\n# torch.save(model.state_dict(), 'image_to_latex_model.pth')","metadata":{"_uuid":"719801b8-1411-43eb-b0b1-9d3808bd9f48","_cell_guid":"8d43228f-181d-4b0e-abed-623e0ad756b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-08T10:57:08.970105Z","iopub.status.idle":"2025-04-08T10:57:08.970372Z","shell.execute_reply":"2025-04-08T10:57:08.970266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load the entire model\n# loaded_model = torch.load('image_to_latex_full_model.pth')\n# loaded_model.eval()  # Set to evaluation mode\n\n# # Or load just the parameters into a model instance\n# model = ImageToLatexModel(vocab_size)  # Create model instance\n# model.load_state_dict(torch.load('image_to_latex_model.pth'))\n# model.eval()\n\n# Or load a checkpoint with additional info\ncheckpoint = torch.load('best_model.pth')\nmodel = ImageToLatexModel(vocab_size)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nstart_epoch = checkpoint['epoch'] + 1\nvocab = checkpoint['vocab']","metadata":{"_uuid":"870630a0-eef9-456a-83b8-f8ddeda746b9","_cell_guid":"7019fb24-213b-4fe1-960b-62c86ee31854","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T04:58:17.041801Z","iopub.execute_input":"2025-04-09T04:58:17.042166Z","iopub.status.idle":"2025-04-09T04:58:18.218925Z","shell.execute_reply.started":"2025-04-09T04:58:17.042130Z","shell.execute_reply":"2025-04-09T04:58:18.218026Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Example: Save model weights\ntorch.save(model.state_dict(), '/kaggle/working/handwritten_to_latex_model.pth')","metadata":{"_uuid":"eefcdfc6-d7a7-46b2-8f13-bc99e4ee09af","_cell_guid":"5d82d45c-a5da-4e87-94b2-cbee7116ec59","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T05:03:17.181787Z","iopub.execute_input":"2025-04-09T05:03:17.182171Z","iopub.status.idle":"2025-04-09T05:03:17.492540Z","shell.execute_reply.started":"2025-04-09T05:03:17.182143Z","shell.execute_reply":"2025-04-09T05:03:17.491373Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ImageToLatexModel(vocab_size)  # Replace with your actual model class\nmodel.load_state_dict(torch.load('/kaggle/working/handwritten_to_latex_model.pth'))","metadata":{"_uuid":"3b04d3df-bd34-41c1-ac56-218b24592753","_cell_guid":"e68aca6e-d93f-4ad5-b119-b66d63b353df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T05:03:19.531029Z","iopub.execute_input":"2025-04-09T05:03:19.531364Z","iopub.status.idle":"2025-04-09T05:03:20.200438Z","shell.execute_reply.started":"2025-04-09T05:03:19.531338Z","shell.execute_reply":"2025-04-09T05:03:20.199410Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_latex(image_path, model, tokenizer, device):\n    # Prepare image\n    image = Image.open(image_path).convert('L')\n    image = image_transform(image).unsqueeze(0).to(device)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Generate prediction\n    with torch.no_grad():\n        outputs = model(image)\n        predicted_indices = outputs.argmax(dim=2).squeeze(0)\n    \n    # Decode prediction\n    predicted_latex = tokenizer.decode(predicted_indices.cpu().numpy())\n    return predicted_latex","metadata":{"_uuid":"d92afb9a-1a17-4b3b-b1e0-658d7892edc8","_cell_guid":"ac51291d-e030-4ef5-9af8-c00896bf36c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-08T20:17:37.345230Z","iopub.execute_input":"2025-04-08T20:17:37.345571Z","iopub.status.idle":"2025-04-08T20:17:37.350816Z","shell.execute_reply.started":"2025-04-08T20:17:37.345543Z","shell.execute_reply":"2025-04-08T20:17:37.349673Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_latex(model, image_path, tokenizer, device, max_length=60):\n    \"\"\"\n    Generate LaTeX code for a given image using the trained model\n    \n    Args:\n        model: Trained ImageToLatexModel\n        image_path: Path to the image file\n        tokenizer: LaTeX tokenizer with encode/decode methods\n        device: Device to run inference on (CPU/GPU)\n        max_length: Maximum output sequence length\n        \n    Returns:\n        predicted_latex: String containing the predicted LaTeX code\n    \"\"\"\n    # Prepare image\n    image_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n    \n    image = Image.open(image_path).convert('L')\n    image = image_transform(image).unsqueeze(0).to(device)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Generate prediction\n    with torch.no_grad():\n        # Initialize first token as <SOS>\n        current_token_idx = torch.ones(1, dtype=torch.long).to(device) * tokenizer.token_to_idx[\"<|SOS|>\"]\n        \n        # Initialize decoder states\n        encoder_features = model.encoder(image)\n        hidden_state = torch.zeros(1, 512).to(device)\n        cell_state = torch.zeros(1, 512).to(device)\n        \n        # Store generated tokens\n        generated_tokens = [current_token_idx.item()]\n        \n        # Generate sequence token by token\n        for _ in range(max_length):\n            # Get next token prediction\n            output, hidden_state, cell_state = model.decoder(\n                current_token_idx, encoder_features, hidden_state, cell_state\n            )\n            \n            # Get most probable token\n            current_token_idx = output.argmax(dim=1)\n            \n            # Add to generated tokens\n            generated_tokens.append(current_token_idx.item())\n            \n            # Stop if <EOS> token is generated\n            if current_token_idx.item() == tokenizer.token_to_idx[\"<|EOS|>\"]:\n                break\n        \n        # Decode the generated tokens\n        predicted_latex = tokenizer.decode(generated_tokens)\n        \n    return predicted_latex\n\n# Load the best model\ndef load_model(model_path, vocab_size):\n    \"\"\"\n    Load a saved model from checkpoint\n    \n    Args:\n        model_path: Path to the model checkpoint\n        vocab_size: Size of the vocabulary\n        \n    Returns:\n        model: Loaded model\n        tokenizer: Tokenizer with vocabulary from checkpoint\n    \"\"\"\n    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n    \n    model = ImageToLatexModel(vocab_size)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Create tokenizer with saved vocabulary\n    vocab = checkpoint['vocab']\n    tokenizer = LatexTokenizer([], max_seq_len=60)  # Create empty tokenizer\n    tokenizer.vocab = vocab  # Set vocabulary from checkpoint\n    tokenizer.token_to_idx = vocab\n    tokenizer.idx_to_token = tokenizer.create_inverse_vocab(vocab)\n    \n    return model, tokenizer\n\n# Example usage\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load model and tokenizer\n    model_path = 'best_model.pth'\n    model, tokenizer = load_model(model_path, vocab_size)\n    model.to(device)\n    \n    # Make prediction on test image\n    test_image_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images/10016fd166.png\"\n    predicted_latex = predict_latex(model, test_image_path, tokenizer, device)\n    \n    print(f\"Predicted LaTeX: {predicted_latex}\")","metadata":{"_uuid":"bb7b6b4e-bc3b-46ae-9501-324ec4c3c2ab","_cell_guid":"07f33b41-6d17-422a-88cd-ed2f0503c47e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-08T20:21:09.050144Z","iopub.execute_input":"2025-04-08T20:21:09.050423Z","iopub.status.idle":"2025-04-08T20:21:10.149947Z","shell.execute_reply.started":"2025-04-08T20:21:09.050402Z","shell.execute_reply":"2025-04-08T20:21:10.148881Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport re\n\n# Load the test data\ntest_csv = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/test.csv\"\ntest_data = pd.read_csv(test_csv)\ntest_image_folder = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n\n# Define image transformation\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load model and tokenizer\ndef load_model(model_path, vocab_size):\n    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n    \n    model = ImageToLatexModel(vocab_size)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Create tokenizer with saved vocabulary\n    vocab = checkpoint['vocab']\n    tokenizer = LatexTokenizer([], max_seq_len=100)\n    tokenizer.vocab = vocab\n    tokenizer.token_to_idx = vocab\n    tokenizer.idx_to_token = tokenizer.create_inverse_vocab(vocab)\n    \n    return model, tokenizer, vocab\n\n# Predict LaTeX for an image\ndef predict_latex(model, image_path, tokenizer, device, max_length=60):\n    # Prepare image\n    image = Image.open(image_path).convert('L')\n    image = image_transform(image).unsqueeze(0).to(device)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Generate prediction\n    with torch.no_grad():\n        # Initialize first token as <SOS>\n        current_token_idx = torch.ones(1, dtype=torch.long).to(device) * tokenizer.token_to_idx[\"<|SOS|>\"]\n        \n        # Initialize decoder states\n        encoder_features = model.encoder(image)\n        hidden_state = torch.zeros(1, 512).to(device)\n        cell_state = torch.zeros(1, 512).to(device)\n        \n        # Store generated tokens\n        generated_tokens = [current_token_idx.item()]\n        \n        # Generate sequence token by token\n        for _ in range(max_length):\n            # Get next token prediction\n            output, hidden_state, cell_state = model.decoder(\n                current_token_idx, encoder_features, hidden_state, cell_state\n            )\n            \n            # Get most probable token\n            current_token_idx = output.argmax(dim=1)\n            \n            # Add to generated tokens\n            generated_tokens.append(current_token_idx.item())\n            \n            # Stop if <EOS> token is generated\n            if current_token_idx.item() == tokenizer.token_to_idx[\"<|EOS|>\"]:\n                break\n        \n        # Decode the generated tokens\n        predicted_latex = tokenizer.decode(generated_tokens)\n        \n    return predicted_latex\n\n# Tokenize LaTeX for BLEU score calculation\ndef tokenize_latex(latex_string):\n    token_pattern = r\"\\\\[a-zA-Z]+|[{}]|[0-9]+|[^\\s]\"\n    tokens = re.findall(token_pattern, latex_string)\n    return tokens\n\n# Calculate BLEU scores\ndef calculate_bleu(reference, candidate):\n    # Tokenize reference and candidate\n    reference_tokens = tokenize_latex(reference)\n    candidate_tokens = tokenize_latex(candidate)\n    \n    # Calculate BLEU score with smoothing\n    smoothie = SmoothingFunction().method1\n    bleu1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n    bleu2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n    bleu3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n    bleu4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n    \n    return {\n        'BLEU-1': bleu1,\n        'BLEU-2': bleu2,\n        'BLEU-3': bleu3,\n        'BLEU-4': bleu4\n    }\n\n# Main execution\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load model and tokenizer\n    model_path = '/kaggle/working/best_model.pth'\n    model, tokenizer, vocab = load_model(model_path, len(vocab))\n    model.to(device)\n    \n    # Keep track of all BLEU scores\n    all_bleu_scores = []\n    \n    # Number of test samples to evaluate (set to a small number for testing, then increase)\n    num_samples = 20\n    \n    # Calculate BLEU score for each test sample\n    for i in range(min(num_samples, len(test_data))):\n        image_name = test_data.iloc[i, 0]\n        ground_truth = test_data.iloc[i, 1]\n        \n        image_path = os.path.join(test_image_folder, image_name)\n        \n        # Get prediction\n        predicted_latex = predict_latex(model, image_path, tokenizer, device)\n        \n        # Calculate BLEU scores\n        bleu_scores = calculate_bleu(ground_truth, predicted_latex)\n        all_bleu_scores.append(bleu_scores)\n        \n        print(f\"Sample {i+1}:\")\n        print(f\"Image: {image_name}\")\n        print(f\"Ground Truth: {ground_truth}\")\n        print(f\"Prediction: {predicted_latex}\")\n        print(f\"BLEU Scores: {bleu_scores}\")\n        print(\"-\" * 50)\n    \n    # Calculate average BLEU scores\n    avg_bleu1 = np.mean([score['BLEU-1'] for score in all_bleu_scores])\n    avg_bleu2 = np.mean([score['BLEU-2'] for score in all_bleu_scores])\n    avg_bleu3 = np.mean([score['BLEU-3'] for score in all_bleu_scores])\n    avg_bleu4 = np.mean([score['BLEU-4'] for score in all_bleu_scores])\n    \n    print(\"\\nAverage BLEU Scores:\")\n    print(f\"BLEU-1: {avg_bleu1:.4f}\")\n    print(f\"BLEU-2: {avg_bleu2:.4f}\")\n    print(f\"BLEU-3: {avg_bleu3:.4f}\")\n    print(f\"BLEU-4: {avg_bleu4:.4f}\")","metadata":{"_uuid":"ea1b9c1e-8299-4765-b47f-732e32da1871","_cell_guid":"f3c6ec97-082c-490d-bd7e-06930a359b59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-09T05:20:54.372215Z","iopub.execute_input":"2025-04-09T05:20:54.372580Z","iopub.status.idle":"2025-04-09T05:20:56.165132Z","shell.execute_reply.started":"2025-04-09T05:20:54.372544Z","shell.execute_reply":"2025-04-09T05:20:56.164488Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-22-8c0dd561b714>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Sample 1:\nImage: 1cb0b785da.png\nGround Truth: $ \\mathcal { F } _ { \\mathrm { i n } } ^ { ( 0 ) } = - S [ T _ { + } ^ { 2 } - C ^ { a } C ^ { a } ] $\nPrediction: $ { \\cal F }_ { \\mathrm { T } } ^ {( 2) } = - [ T ^ { a } T ^ { a } - S ^ { 2 } T ^ { a } ] $\nBLEU Scores: {'BLEU-1': 0.8220130616218343, 'BLEU-2': 0.697038298428997, 'BLEU-3': 0.5801888111947915, 'BLEU-4': 0.47385478523101565}\n--------------------------------------------------\nSample 2:\nImage: 6f2229183a.png\nGround Truth: $ d ( l _ { 0 } + 1 , k _ { 1 } ; l _ { 0 } , k _ { 1 } ) c ( l _ { 0 } , k _ { 1 } ; l _ { 0 } + 1 , k _ { 1 } ) = \\frac { ( k _ { 1 } + k _ { 0 } ) a - ( k _ { 1 } - k _ { 0 } ) b } { 2 k _ { 1 } ( k _ { 1 } - l _ { 0 } ) ( k _ { 1 } + l _ { 0 } + 1 ) } , $\nPrediction: $ d(( k_ { 0 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_ { 1 }, k_\nBLEU Scores: {'BLEU-1': 0.30049438676849755, 'BLEU-2': 0.27761370272271946, 'BLEU-3': 0.24944306838886127, 'BLEU-4': 0.22014615105130464}\n--------------------------------------------------\nSample 3:\nImage: 49e5037a24.png\nGround Truth: $ \\chi _ { k , l } ( q , \\theta ) ~ \\chi _ { 1 , \\epsilon } ( q , \\theta ) = \\sum _ { l ^ { \\prime } } \\chi _ { k + 1 , l ^ { \\prime } } ( q , \\theta ) \\chi _ { c , \\Delta _ { 2 l + 1 , 2 l ^ { \\prime } + 1 } } ^ { V } ( q ) $\nPrediction: $ \\chi_ { k, }( q, \\theta, \\chi) \\chi_ { k, l }( \\theta, \\chi) = \\sum_ { k, l } ^ { { k + 1 } } \\chi_ { k, l + 1 }( \\chi, \\chi) + \\chi_ {\nBLEU Scores: {'BLEU-1': 0.5823292233437252, 'BLEU-2': 0.47721522604788014, 'BLEU-3': 0.40872984254620165, 'BLEU-4': 0.33406174795754534}\n--------------------------------------------------\nSample 4:\nImage: 7f06160259.png\nGround Truth: $ ( F _ { \\mu \\nu } ) = \\left( \\begin{array} { c c c } { 0 } & { B } & { E _ { 1 } } \\\\ { - B } & { 0 } & { E _ { 2 } } \\\\ { - E _ { 1 } } & { - E _ { 2 } } & { 0 } \\\\ \\end{array} \\right) = i ( E _ { 2 } T _ { 1 } - E _ { 1 } T _ { 2 } + B T _ { 3 } ) \\ , $\nPrediction: $((_ { \\mu \\nu }) = \\left( \\begin { a r r a y } { c c c c } { - } & { 0 } & { 0 } & { - } } \\ \\ { 0 } & { -_ { 1 } } & { 0 } \\\nBLEU Scores: {'BLEU-1': 0.31122632915308673, 'BLEU-2': 0.2943045937589128, 'BLEU-3': 0.27277146401822605, 'BLEU-4': 0.25313986815456124}\n--------------------------------------------------\nSample 5:\nImage: 6cd2e6c9cb.png\nGround Truth: $ B ^ { 1 \\alpha , 2 \\beta } \\left( \\partial _ { \\tau } X ^ { 1 \\alpha } \\partial _ { \\sigma } X ^ { 2 \\beta } - \\partial _ { \\tau } X ^ { 2 \\beta } \\partial _ { \\sigma } X ^ { 1 \\alpha } \\right) . $\nPrediction: $ B ^ { \\alpha \\beta } } \\partial_ { \\tau } X ^ { \\alpha \\beta } \\left( \\partial_ { \\tau } X ^ { \\alpha } \\partial_ { \\tau } X ^ { \\beta } \\partial_ { \\tau } X ^ { \\beta } \\partial ^ { 2 } X ^ { \\alpha\nBLEU Scores: {'BLEU-1': 0.7867771630572942, 'BLEU-2': 0.6965970137488776, 'BLEU-3': 0.6231132135554154, 'BLEU-4': 0.5422509160219594}\n--------------------------------------------------\nSample 6:\nImage: 2930165093.png\nGround Truth: $ V [ \\zeta ] = V _ { \\rho } [ \\zeta ] + V _ { a } [ \\zeta ] + V _ { 0 } [ \\zeta ] \\; . $\nPrediction: $ V [ Q ] = [ V_ { 0 } [ V ] + [ V_ { 0 } [ V ] + V_ { 0 } [ V ] +. $\nBLEU Scores: {'BLEU-1': 0.7567567567567568, 'BLEU-2': 0.6151246852292456, 'BLEU-3': 0.46352510004173414, 'BLEU-4': 0.34585811169370456}\n--------------------------------------------------\nSample 7:\nImage: 71a39fa067.png\nGround Truth: $ { \\cal S } M = ( I \\times M ) / ( \\{ 0 \\} \\times M \\cup \\{ 1 \\} \\times M ) . $\nPrediction: $ { \\cal M } =( M) \\ { M( M) } \\ { M \\ } \\ M \\ } \\ \\ M) \\ }. $\nBLEU Scores: {'BLEU-1': 0.696969696969697, 'BLEU-2': 0.46669372215943755, 'BLEU-3': 0.1947205259393578, 'BLEU-4': 0.06956571325827682}\n--------------------------------------------------\nSample 8:\nImage: 4ec3f79cab.png\nGround Truth: $ \\hat { T } = \\sum _ { \\{ \\Delta _ { 0 } S _ { \\gamma } \\} } \\prod _ { p \\in \\Delta _ { 0 } S _ { \\gamma } } \\hat { W } _ { p } ^ { n _ { p } } \\operatorname { e x p } { [ \\, - \\frac { 1 } { 2 \\beta _ { 0 } } \\sum _ { \\ell } \\hat { E } _ { \\ell } ^ { 2 } - \\frac { 1 } { 2 \\beta _ { s } } \\sum _ { p _ { s } \\in ( S _ { t + a _ { 0 } } - S _ { t } ) } n _ { p } ^ { 2 } \\, ] \\, } , $\nPrediction: $ \\hat { T }: = \\sum_ { { \\in \\Delta } {_ { { } } { { } } { { } } { { } } { { } } { { } } { { } } { { } } { { } } { { } } { { } }\nBLEU Scores: {'BLEU-1': 0.20186816328691073, 'BLEU-2': 0.1124417762790277, 'BLEU-3': 0.06145340453689264, 'BLEU-4': 0.039431607884012045}\n--------------------------------------------------\nSample 9:\nImage: 6f26c774d9.png\nGround Truth: $ \\eta = { \\frac { \\lambda ^ { 6 } } { \\kappa ^ { 4 } } } = ( 4 \\pi ) ^ { 5 } , $\nPrediction: $ \\eta = \\frac { \\lambda ^ { 4 } } { 4 \\kappa ^ { 4 } } =( 4 \\pi) ^ { 4 }, $\nBLEU Scores: {'BLEU-1': 0.8704944904338052, 'BLEU-2': 0.7992030147251229, 'BLEU-3': 0.7232986998838321, 'BLEU-4': 0.6200408002607087}\n--------------------------------------------------\nSample 10:\nImage: 5002192b47.png\nGround Truth: $ { \\cal D } = \\left( \\begin{array} { c c c c c c c c } { 1 } & { \\partial } & { \\cdots } & { \\partial ^ { n } } & { D } & { D \\partial } & { \\cdots } & { D \\partial ^ { n - 1 } } \\\\ \\end{array} \\right) ^ { s t } . $\nPrediction: $ { \\cal D } = \\left( 1 \\cdot \\partial \\cdot \\partial \\cdot \\partial \\cdot B) ^ { n } \\, \\ \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\\nBLEU Scores: {'BLEU-1': 0.19857821033646775, 'BLEU-2': 0.1656441441804797, 'BLEU-3': 0.14083705987376435, 'BLEU-4': 0.11521934240470799}\n--------------------------------------------------\nSample 11:\nImage: 4abaeefebb.png\nGround Truth: $ \\sum _ { \\alpha , A } b _ { a } ^ { \\alpha } n _ { \\alpha } ^ { A } + \\sum _ { A } b _ { \\alpha } ^ { \\prime } n _ { \\alpha } ^ { A } = \\sum _ { A } { \\frac { C _ { a } ^ { A } } { 4 \\pi ^ { 2 } } } . $\nPrediction: $ \\sum_ { \\alpha } \\alpha_ { \\alpha } ^ { A } v_ { A } ^ { A } + \\sum_ { \\alpha } ^ { A } {_ { A } ^ { A } = \\sum_ { \\alpha } \\frac { A_ { A } ^ { A }\nBLEU Scores: {'BLEU-1': 0.6687625565355366, 'BLEU-2': 0.6112325462547932, 'BLEU-3': 0.5512198098894316, 'BLEU-4': 0.4827408732816072}\n--------------------------------------------------\nSample 12:\nImage: 7723f4b1b6.png\nGround Truth: $ \\phi ( { \\bf x } , t ) = \\sum _ { \\alpha } \\phi _ { \\alpha } ( t ) u _ { \\alpha } ( { \\bf x } ) , \\pi ( { \\bf x } , t ) = \\sum _ { \\alpha } \\pi _ { \\alpha } ( t ) u _ { \\alpha } ( { \\bf x } ) . $\nPrediction: $ \\phi( t, t) = \\sum_ { \\alpha }(_ { \\alpha }( { \\bf x }, t) t) {_ { \\alpha }( { \\bf x }, t) = \\sum_ { \\alpha } t_ { \\alpha }( t). $\nBLEU Scores: {'BLEU-1': 0.7281668146677497, 'BLEU-2': 0.6853719811856919, 'BLEU-3': 0.6415249782657954, 'BLEU-4': 0.6013981098440719}\n--------------------------------------------------\nSample 13:\nImage: 26b39ea7f5.png\nGround Truth: $ V = \\Phi ^ { n - 1 } ( x , \\eta ) \\Phi ( x , - ( n - 1 ) \\eta ) $\nPrediction: $ V = \\Phi ^ { n - 1 }( x, y) - 1)( x - y) $\nBLEU Scores: {'BLEU-1': 0.7759415811497294, 'BLEU-2': 0.6759574815679117, 'BLEU-3': 0.597402583181816, 'BLEU-4': 0.5400118628570391}\n--------------------------------------------------\nSample 14:\nImage: 609184ecf9.png\nGround Truth: $ \\Delta ^ { + } = \\Delta ^ { - } = \\displaystyle \\frac { ( 2 n p + l ) ^ { 2 } - 1 } { 4 p ( p + 1 ) } \\, \\, . $\nPrediction: $ \\Delta ^ { + } = \\Delta ^ { 2 } = \\frac {( 2 p) } { 2 } } {( p + 1) } {( p + 1) } \\;. $\nBLEU Scores: {'BLEU-1': 0.7758856497533524, 'BLEU-2': 0.6502824526087264, 'BLEU-3': 0.5539474127982056, 'BLEU-4': 0.4773725243582147}\n--------------------------------------------------\nSample 15:\nImage: 46de317e8c.png\nGround Truth: $ \\psi _ { 0 } \\sim \\ell \\, \\big ( | z _ { 1 } | / \\ell + | z _ { 2 } | / \\ell + 1 \\big ) ^ { - 2 } \\, . $\nPrediction: $ \\psi_ { 0 } | \\sim( z_ { 1 } | z | + 2 z_ { 1 } | z 2) ^ { - 1/ 2 } | ^ { - 1/ 2 }. $\nBLEU Scores: {'BLEU-1': 0.7555555555555555, 'BLEU-2': 0.600504838120444, 'BLEU-3': 0.46871204905364683, 'BLEU-4': 0.3598724830175526}\n--------------------------------------------------\nSample 16:\nImage: 2cb92eb318.png\nGround Truth: $ M ^ { i j } ( x , y ) = D _ { \\mu } ^ { i k } [ A ] D _ { \\nu } ^ { k j } [ A + Q ] ( x ) \\delta ( x - y ) $\nPrediction: $ M ^ { i j }( x) = D_ { \\mu } ^ { i j } [ A_ { \\mu } ^ { i } [ A ] + \\delta( x) y) ] [ A( x) ] $\nBLEU Scores: {'BLEU-1': 0.84245375459746, 'BLEU-2': 0.7227200622444986, 'BLEU-3': 0.6195844659773477, 'BLEU-4': 0.5206387219741916}\n--------------------------------------------------\nSample 17:\nImage: 5061724c3f.png\nGround Truth: $ f ( x ) = \\int _ { - \\infty } ^ { \\infty } \\frac { d \\xi } { 2 \\pi } \\mathrm { e } ^ { i ( 1 - x ) \\, \\xi } \\langle \\widetilde { H } ( p ) | W _ { \\scriptstyle \\Pi } ( { v \\! \\cdot \\! n } \\, \\xi \\mu - i 0 ) | \\widetilde { H } ( p ) \\rangle \\, , $\nPrediction: $ f( x) = \\int_ { - \\infty } ^ { \\infty } \\frac { d ^ { { - i p } { { {( } } { { {( }( \\hat { H }(() p( { - i p( { \\hat { H }(()\nBLEU Scores: {'BLEU-1': 0.4994753854370558, 'BLEU-2': 0.3956523777494171, 'BLEU-3': 0.33279835312369616, 'BLEU-4': 0.29207061776316273}\n--------------------------------------------------\nSample 18:\nImage: 88fd19fd2d.png\nGround Truth: $ t _ { i } = t + \\zeta t , \\, \\, \\, \\zeta = \\zeta ( i ) $\nPrediction: $ t_ { i } = t + \\zeta \\zeta \\zeta \\zeta \\zeta \\zeta \\zeta( \\zeta) = \\zeta $\nBLEU Scores: {'BLEU-1': 0.6345638493595809, 'BLEU-2': 0.5385343024656297, 'BLEU-3': 0.46897009608733514, 'BLEU-4': 0.4247142237775684}\n--------------------------------------------------\nSample 19:\nImage: 4e723d9918.png\nGround Truth: $ \\overline { { g } } _ { \\mu \\nu } = \\left( \\begin{array} { c c } { - 1 \\, } & { \\, 0 } \\\\ { 0 \\, } & { \\, e ^ { 2 H t } \\delta _ { i j } } \\\\ \\end{array} \\right) $\nPrediction: $ \\bar { g }_ { \\mu \\nu } = \\left( \\begin { a r r a y } { c c } { 0 } & { - 2 } \\ \\ { 0 } & { - e ^ { - 2 i \\theta } } \\ \\ { 0 } & { - e ^\nBLEU Scores: {'BLEU-1': 0.6277238803788806, 'BLEU-2': 0.5518551073640757, 'BLEU-3': 0.48800875123103465, 'BLEU-4': 0.42733349906773194}\n--------------------------------------------------\nSample 20:\nImage: 26276acc85.png\nGround Truth: $ z _ { \\check { A } } ^ { \\mu } ( \\tau , \\vec { \\sigma } ) = \\partial _ { \\check { A } } z ^ { \\mu } ( \\tau , \\vec { \\sigma } ) , \\quad \\quad \\partial _ { \\check { B } } z _ { \\check { A } } ^ { \\mu } - \\partial _ { \\check { A } } z _ { \\check { B } } ^ { \\mu } = 0 , $\nPrediction: $ z_ { A } ^ { \\mu }( z, \\bar { z }) = \\partial_ { \\bar { z } } ^ { \\mu }( z, \\bar { z }), \\qquad \\partial_ { \\bar { z } } ^ { \\mu } \\partial_ { \\bar { z }\nBLEU Scores: {'BLEU-1': 0.4970879914124737, 'BLEU-2': 0.4010263209234213, 'BLEU-3': 0.33825325422889274, 'BLEU-4': 0.273232246861524}\n--------------------------------------------------\n\nAverage BLEU Scores:\nBLEU-1: 0.6167\nBLEU-2: 0.5218\nBLEU-3: 0.4389\nBLEU-4: 0.3706\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}